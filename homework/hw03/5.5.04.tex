\begin{problem}
  \probNum{5.5.4}~Let ${Y_1,Y_2,\ldots,Y_n}$ be a random sample from the uniform pdf ${f_Y(y;\theta) = 1 / \theta \text{, } 0 \leq y \leq \theta}$. Compare the Cramer-Rao lower bound for~${f_Y(y;\theta)}$ with the variance of the unbiased estimator ${\hat{\theta} = \frac{n+1}{n} Y_{\max}}$. Discuss.
\end{problem}

\begin{align}
  \ln f_Y &= \ln \left(\frac{1}{\theta}\right) \\
  \frac{\partial \ln f_Y}{\partial \theta} &= -\frac{1}{\theta} %\\
  % \frac{\partial^2 \ln f_Y}{\partial \theta^2} &= \frac{1}{\theta^2}
\end{align}

\begin{align}
  \expect{\left(\frac{\partial \ln f_Y}{\partial \theta} \right)^2} = \expect{\frac{1}{\theta^2}} &= \int_{0}^{\theta}\frac{1}{\theta^3}dy \\
                              &= \frac{1}{\theta^2}
\end{align}

This makes the Cramer-Rao lower bound $\boxed{\frac{\theta^2}{n}}$.

In problem~5.4.18, we determined the estimator's variance was~$\frac{\theta^2}{n(n+1)}$ which is less than the Cramer-Rao lower bound.  Note that Theorem~5.5.1 specifics that the lower bound only applies when the support does not depend on~$\theta$ as it does in this case.  Therefore, the lower bound does not apply.
